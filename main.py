# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_0ecgW6E87faMuRIBO23o7hwNNCR0tRI
"""

import pandas as pd
import numpy as np
import re
import math

"""Read DataSet:"""
data = pd.read_csv("cars.csv")
try:
    initial_count = len(data)
    print("Cars Dataset Overview:")
    print("The First 5 Cars:")
    print(data.head().to_string())
    print("Dataset Statistics:")
    print(f"The total number of cars: {len(data):,}")
    print(f"Number of unique brands: {data['brand'].nunique()}")
    print(f"Countries represented: {', '.join(data['country'].unique())}")
    """ Data Preprocessing Steps: """
    #1)Removes Duplicate Rows:
    data = data.drop_duplicates()
except FileNotFoundError:
    print("File Not Found ... ")
except Exception as e:
    print(f"An Error Occurred: {e}")

#2)Fill Missing Values in 'cylinder':
data['cylinder'] = data['cylinder'].replace(["N/A, Electric", "Single","Drive Type"], None)
data['cylinder'] = data['cylinder'].fillna(data['cylinder'].mode()[0])
#3)Fill Missing Values in 'horse_power':
data['horse_power'] = pd.to_numeric(data['horse_power'].str.replace(r'[^\d.]', '', regex=True), errors='coerce')
data['horse_power'] = data['horse_power'].fillna(data['horse_power'].median())

#4)
data['price'] = data['price'].astype(str)
data['price'] = data['price'].replace(['DISCONTINUED', 'TBD', 'Following', 'N/A'], pd.NA)
data['price'] = data['price'].apply(
    lambda x: str(x).translate(str.maketrans('٠١٢٣٤٥٦٧٨٩', '0123456789')) if pd.notna(x) else x)
data['price'] = data['price'].str.replace(r'[^\d.]', '', regex=True)
data['price'] = pd.to_numeric(data['price'], errors='coerce')
"""The currency_rates dictionary contains USD exchange rates"""
currency_rates = {
    'ksa': 0.27, 'egypt': 0.032, 'bahrain': 2.65, 'qatar': 0.27,
    'oman': 2.60, 'kuwait': 3.25, 'uae': 0.27
}
for country, rate in currency_rates.items():
    country_mask = data['country'].str.lower() == country.lower()
    data.loc[country_mask, 'price'] = data.loc[country_mask, 'price'] * rate
##Fill Missing 'price' values:
type_means = data.groupby('brand')['price'].transform('mean')
data['price'] = data['price'].fillna(type_means)
overall_mean = data['price'].mean()
data['price'] = data['price'].fillna(overall_mean)

#5)Fill Missing 'engine_capacity' Values:
data['engine_capacity'] = pd.to_numeric(data['engine_capacity'], errors='coerce')
data.loc[data['engine_capacity'] == 0, 'engine_capacity'] = np.nan
data.loc[data['engine_capacity'] < 15, 'engine_capacity'] *= 1000
mean_engine_capacity = data['engine_capacity'][data['engine_capacity'] > 0].median()
data['engine_capacity'] = data['engine_capacity'].fillna(mean_engine_capacity)

#6)Fill Missing 'engine_capacity' Values:
data['top_speed'] = data['top_speed'].replace('N A', np.nan)
data['top_speed'] = pd.to_numeric(data['top_speed'], errors='coerce')
brand_means = data.groupby('brand')['top_speed'].mean()
def replace_with_brand_mean(row):
    if pd.isna(row['top_speed']):
        return brand_means[row['brand']]
    elif isinstance(row['top_speed'], str):
        return brand_means[row['brand']]
    else:
        return row['top_speed']
data['top_speed'] = data.apply(replace_with_brand_mean, axis=1)

#7)Clean 'seats' column:
def clean_seats(value, max_limit=12):
    if isinstance(value, str):
        match = re.match(r"(\d+\.?\d*)", value)
        if match:
            num = float(match.group(1))
            if num > max_limit:
                return np.nan
            return math.ceil(num)
        else:
            return np.nan
    elif isinstance(value, (int, float)):
        if value > max_limit:
            return np.nan
        return math.ceil(value)
    return np.nan
data['seats'] = data['seats'].apply(clean_seats)
data['seats'] = data['seats'].fillna(data['seats'].mean())

##Export cleaned data to CSV:
output_file = 'cleaned_cars_dataset.csv'
data.to_csv(output_file, index=False)
print(f"\nCleaned Data Has Been Exported To: {output_file}")
##Display First Few Rows Of Cleaned Data:
print("First few rows of cleaned data:")
print(data.head().to_string())

data['brand'].value_counts().plot.barh(figsize=(10,20));

data['country'].value_counts().plot.barh(figsize=(4,4));

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from scipy import stats

data = pd.read_csv("cleaned_cars_dataset.csv")

if 'car name' in data.columns:
    data = data.drop('car name', axis=1)


## Function To Remove Outliers Using Z-Scores:
def remove_outliers(df, column, n_sigmas=3):
    z_scores = stats.zscore(df[column])
    abs_z_scores = np.abs(z_scores)
    filtered_entries = (abs_z_scores < n_sigmas)

    return df[filtered_entries]


data['price'] = pd.to_numeric(data['price'], errors='coerce')
data = data.dropna(subset=['price'])
data = remove_outliers(data, 'price')


## Handle Numeric Columns With MinMaxScaler:
numeric_columns = ['top_speed', 'engine_capacity', 'horse_power']
scaler = MinMaxScaler(feature_range=(-1, 1))  # Scale to [-1, 1] for RBF
for col in numeric_columns:

    data[col] = pd.to_numeric(data[col], errors='coerce')

    median_val = data[col].median()
    data[col] = data[col].fillna(median_val)
    data = remove_outliers(data, col)


data[numeric_columns] = scaler.fit_transform(data[numeric_columns])

## Normalizing:
data['cylinder'] = pd.to_numeric(data['cylinder'].replace({
    'N/A, Electric': '0',
    'Single': '1',
    'Twin': '2'
}).astype(str), errors='coerce')

data['cylinder'] = data['cylinder'].fillna(data['cylinder'].median())

data['cylinder'] = (data['cylinder'] - data['cylinder'].min()) / (data['cylinder'].max() - data['cylinder'].min()) * 2 - 1

## Normalizing 'seats' to [-1, 1] range:
data['seats'] = pd.to_numeric(data['seats'], errors='coerce')
data['seats'] = data['seats'].fillna(data['seats'].median())

data['seats'] = data['seats'].clip(2, 9)  # Clip to reasonable values
data['seats'] = (data['seats'] - data['seats'].min()) / (data['seats'].max() - data['seats'].min()) * 2 - 1

## Encoding 'brand' Using Weighted Frequency Encoding:
brand_counts = data['brand'].value_counts()
brand_freq = data['brand'].value_counts(normalize=True)

brand_encoding = brand_freq * np.log1p(brand_counts)
brand_encoding = (brand_encoding - brand_encoding.min()) / (brand_encoding.max() - brand_encoding.min()) * 2 - 1

data['brand'] = data['brand'].map(brand_encoding)

## Encoding 'country' Using Weighted Frequency Encoding:
country_counts = data['country'].value_counts()
country_freq = data['country'].value_counts(normalize=True)
country_encoding = country_freq * np.log1p(country_counts)

country_encoding = (country_encoding - country_encoding.min()) / (country_encoding.max() - country_encoding.min()) * 2 - 1
data['country'] = data['country'].map(country_encoding)

data = data.fillna(0)

## Dislay Data Summary:
print("\nData Summary:")
print("-" * 50)
print(f"Number of samples: {len(data)}")
print("Feature Statistics:")
print(data.describe())

## Check Correlations between "price and other freatures":
print("\nCorrelations with price:")
correlations = data.corr()['price'].sort_values(ascending=False)
print(correlations)

for col in data.columns:
    if col != 'price':
        min_val = data[col].min()
        max_val = data[col].max()

        if min_val < -1 or max_val > 1:
            print(f"{col}: min={min_val}, max={max_val}")

## Save Processed Data In File:
output_file = 'encoded_standardized_cars_dataset.csv'
data.to_csv(output_file, index=False)

print(f"\nProcessed Data Has Been Exported To: {output_file}")

## Dislay Final_DataSet Summary:
print("Final Dataset Shape:", data.shape)
print("Columns In Final DataSet:", data.columns.tolist())

from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def train_regularized_models():
    results = []

    # Define parameter grids for each model
    param_grids = {
        'lasso': {
            'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0],
            'max_iter': [3000],
            'tol': [1e-4, 1e-3]
        },
        'ridge': {
            'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0],
            'tol': [1e-4, 1e-3]
        },
        'polynomial': {
            'polynomialfeatures__degree': [2, 3, 4],
            'linearregression__fit_intercept': [True, False]
        },
        'rbf': {
            'alpha': [0.0001, 0.001, 0.01, 0.1],
            'gamma': [0.0001, 0.001, 0.01, 0.1]
        }
    }

    # 1. LASSO with Grid Search
    print("\nTraining LASSO with Grid Search...")
    lasso_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('lasso', Lasso())
    ])

    lasso_grid = GridSearchCV(
        lasso_pipeline,
        {'lasso__' + key: value for key, value in param_grids['lasso'].items()},
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )

    lasso_grid.fit(X_train, y_train)
    lasso_pred = lasso_grid.predict(X_val)
    lasso_results = evaluate_model(y_val, lasso_pred, "LASSO")
    lasso_results.update({
        'best_params': lasso_grid.best_params_,
        'model': lasso_grid.best_estimator_
    })
    results.append(lasso_results)

    print(f"Best LASSO parameters: {lasso_grid.best_params_}")
    print(f"LASSO R2 Score: {lasso_results['R2']:.4f}")

    # 2. Ridge with Grid Search
    print("\nTraining Ridge with Grid Search...")
    ridge_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('ridge', Ridge())
    ])

    ridge_grid = GridSearchCV(
        ridge_pipeline,
        {'ridge__' + key: value for key, value in param_grids['ridge'].items()},
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )

    ridge_grid.fit(X_train, y_train)
    ridge_pred = ridge_grid.predict(X_val)
    ridge_results = evaluate_model(y_val, ridge_pred, "Ridge")
    ridge_results.update({
        'best_params': ridge_grid.best_params_,
        'model': ridge_grid.best_estimator_
    })
    results.append(ridge_results)

    print(f"Best Ridge parameters: {ridge_grid.best_params_}")
    print(f"Ridge R2 Score: {ridge_results['R2']:.4f}")

    # 3. Polynomial with Regularization
    print("\nTraining Polynomial Regression with Regularization...")
    poly_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('polynomialfeatures', PolynomialFeatures()),
        ('linearregression', LinearRegression())
    ])

    poly_grid = GridSearchCV(
        poly_pipeline,
        param_grids['polynomial'],
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )

    poly_grid.fit(X_train, y_train)
    poly_pred = poly_grid.predict(X_val)
    poly_results = evaluate_model(y_val, poly_pred, "Polynomial")
    poly_results.update({
        'best_params': poly_grid.best_params_,
        'model': poly_grid.best_estimator_
    })
    results.append(poly_results)

    print(f"Best Polynomial parameters: {poly_grid.best_params_}")
    print(f"Polynomial R2 Score: {poly_results['R2']:.4f}")

    # 4. RBF with Grid Search
    print("\nTraining RBF with Grid Search...")
    rbf_model = KernelRidge(kernel='rbf')
    rbf_grid = GridSearchCV(
        rbf_model,
        param_grids['rbf'],
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )

    rbf_grid.fit(X_train, y_train)
    rbf_pred = rbf_grid.predict(X_val)
    rbf_results = evaluate_model(y_val, rbf_pred, "RBF")
    rbf_results.update({
        'best_params': rbf_grid.best_params_,
        'model': rbf_grid.best_estimator_
    })
    results.append(rbf_results)

    print(f"Best RBF parameters: {rbf_grid.best_params_}")
    print(f"RBF R2 Score: {rbf_results['R2']:.4f}")

    return pd.DataFrame(results)

# Function to analyze regularization effects
def analyze_regularization(model, X, feature_names):
    """Analyze the effects of regularization on feature coefficients"""
    if hasattr(model, 'named_steps'):
        if 'lasso' in model.named_steps:
            coef = model.named_steps['lasso'].coef_
        elif 'ridge' in model.named_steps:
            coef = model.named_steps['ridge'].coef_
        else:
            return
    else:
        coef = model.coef_

    # Create feature importance DataFrame
    importance = pd.DataFrame({
        'Feature': feature_names,
        'Coefficient': coef,
        'Abs_Coefficient': np.abs(coef)
    })

    # Count number of zero coefficients (LASSO)
    zero_coef = (np.abs(coef) < 1e-10).sum()

    print(f"\nNumber of zero coefficients: {zero_coef}")
    print("\nTop 10 most important features:")
    print(importance.sort_values('Abs_Coefficient', ascending=False).head(10))

# Train models with regularization
print("Training models with regularization and grid search...")
results_df = train_regularized_models()

# Print overall results
print("\nModel Comparison on Validation Set:")
print(results_df.sort_values('MSE')[['Model', 'MSE', 'MAE', 'R2', 'RMSE']])

# Analyze best model
best_model_info = results_df.loc[results_df['MSE'].idxmin()]
print(f"\nBest model: {best_model_info['Model']}")
print(f"Best parameters: {best_model_info['best_params']}")

# Analyze regularization effects for best model
if hasattr(best_model_info['model'], 'coef_') or hasattr(best_model_info['model'], 'named_steps'):
    analyze_regularization(best_model_info['model'], X_train, X_train.columns)

def display_detailed_results(results_df):
    """
    Display detailed results for all model variations
    """
    print("\n" + "="*100)
    print("DETAILED RESULTS FOR ALL MODEL VARIATIONS")
    print("="*100)

    # Polynomial Results
    print("\nPOLYNOMIAL REGRESSION RESULTS:")
    print("-"*100)
    poly_results = results_df[results_df['Model'].str.contains('Polynomial')]
    if not poly_results.empty:
        for _, row in poly_results.iterrows():
            print(f"\nModel: {row['Model']}")
            print(f"MSE:  {row['MSE']:,.2f}")
            print(f"RMSE: {row['RMSE']:,.2f}")
            print(f"MAE:  {row['MAE']:,.2f}")
            print(f"R2:   {row['R2']:.4f}")
            if 'best_params' in row:
                print("Best Parameters:", row['best_params'])
            print("-"*50)

    # LASSO Results
    print("\nLASSO REGRESSION RESULTS:")
    print("-"*100)
    lasso_results = results_df[results_df['Model'].str.contains('LASSO')]
    if not lasso_results.empty:
        lasso_results = lasso_results.sort_values('MSE')
        for _, row in lasso_results.iterrows():
            print(f"\nModel: {row['Model']}")
            print(f"MSE:  {row['MSE']:,.2f}")
            print(f"RMSE: {row['RMSE']:,.2f}")
            print(f"MAE:  {row['MAE']:,.2f}")
            print(f"R2:   {row['R2']:.4f}")
            if 'best_params' in row:
                print("Best Parameters:", row['best_params'])
            print("-"*50)

    # Ridge Results
    print("\nRIDGE REGRESSION RESULTS:")
    print("-"*100)
    ridge_results = results_df[results_df['Model'].str.contains('Ridge')]
    if not ridge_results.empty:
        ridge_results = ridge_results.sort_values('MSE')
        for _, row in ridge_results.iterrows():
            print(f"\nModel: {row['Model']}")
            print(f"MSE:  {row['MSE']:,.2f}")
            print(f"RMSE: {row['RMSE']:,.2f}")
            print(f"MAE:  {row['MAE']:,.2f}")
            print(f"R2:   {row['R2']:.4f}")
            if 'best_params' in row:
                print("Best Parameters:", row['best_params'])
            print("-"*50)

    # Best Model from Each Type
    print("\nBEST MODEL FROM EACH TYPE:")
    print("-"*100)

    model_types = ['Polynomial', 'LASSO', 'Ridge', 'RBF']
    for model_type in model_types:
        type_results = results_df[results_df['Model'].str.contains(model_type)]
        if not type_results.empty:
            best_model = type_results.loc[type_results['MSE'].idxmin()]
            print(f"\nBest {model_type} Model:")
            print(f"Model: {best_model['Model']}")
            print(f"MSE:  {best_model['MSE']:,.2f}")
            print(f"RMSE: {best_model['RMSE']:,.2f}")
            print(f"MAE:  {best_model['MAE']:,.2f}")
            print(f"R2:   {best_model['R2']:.4f}")
            if 'best_params' in best_model:
                print("Best Parameters:", best_model['best_params'])
            print("-"*50)

    # Overall Best Model
    print("\nOVERALL BEST MODEL:")
    print("-"*100)
    best_overall = results_df.loc[results_df['MSE'].idxmin()]
    print(f"Model: {best_overall['Model']}")
    print(f"MSE:  {best_overall['MSE']:,.2f}")
    print(f"RMSE: {best_overall['RMSE']:,.2f}")
    print(f"MAE:  {best_overall['MAE']:,.2f}")
    print(f"R2:   {best_overall['R2']:.4f}")
    if 'best_params' in best_overall:
        print("Best Parameters:", best_overall['best_params'])

# Train models
print("Training models...")
results_df = train_regularized_models()

# Display detailed results
display_detailed_results(results_df)

# Performance Summary Table
print("\nPERFORMANCE SUMMARY TABLE:")
print("-"*100)
summary_table = pd.DataFrame({
    'Model': results_df['Model'],
    'MSE': results_df['MSE'].round(2),
    'RMSE': results_df['RMSE'].round(2),
    'MAE': results_df['MAE'].round(2),
    'R2': results_df['R2'].round(4)
}).sort_values('MSE')

print(summary_table.to_string(index=False))

def closed_form_linear_regression():
    """
    Implement linear regression using the closed-form solution:
    β = (X^T X)^(-1) X^T y
    """
    print("\nTraining Closed-Form Linear Regression...")
    try:
        # Add bias term to X
        X_train_b = np.column_stack([np.ones(len(X_train)), X_train])
        X_val_b = np.column_stack([np.ones(len(X_val)), X_val])

        # Calculate closed-form solution
        # β = (X^T X)^(-1) X^T y
        XtX = np.dot(X_train_b.T, X_train_b)
        XtX_inv = np.linalg.inv(XtX)
        Xty = np.dot(X_train_b.T, y_train)
        beta = np.dot(XtX_inv, Xty)

        # Make predictions
        train_pred = np.dot(X_train_b, beta)
        val_pred = np.dot(X_val_b, beta)

        # Calculate metrics
        train_metrics = evaluate_model(y_train, train_pred, 'Closed-Form Linear Regression - Train')
        val_metrics = evaluate_model(y_val, val_pred, 'Closed-Form Linear Regression')

        # Create a results dictionary similar to other models
        results = {
            'Model': 'Closed-Form Linear Regression',
            'MSE': val_metrics['MSE'],
            'MAE': val_metrics['MAE'],
            'R2': val_metrics['R2'],
            'RMSE': val_metrics['RMSE'],
            'Train_R2': train_metrics['R2'],
            'coefficients': beta,
            'is_closed_form': True
        }

        # Print detailed results
        print("\nClosed-Form Linear Regression Results:")
        print(f"Training R2: {train_metrics['R2']:.4f}")
        print(f"Validation R2: {val_metrics['R2']:.4f}")
        print(f"Training MSE: {train_metrics['MSE']:.4f}")
        print(f"Validation MSE: {val_metrics['MSE']:.4f}")

        # Print feature importances
        feature_names = ['bias'] + X_train.columns.tolist()
        importance = pd.DataFrame({
            'Feature': feature_names,
            'Coefficient': beta
        })
        importance['Abs_Coefficient'] = abs(importance['Coefficient'])
        importance = importance.sort_values('Abs_Coefficient', ascending=False)

        print("\nTop 10 most important features (Closed-Form):")
        print(importance.head(10))

        return results

    except np.linalg.LinAlgError as e:
        print(f"Error in closed-form solution: {str(e)}")
        print("Matrix might be singular or poorly conditioned")
        return None
    except Exception as e:
        print(f"Unexpected error in closed-form solution: {str(e)}")
        return None

# Modify train_all_models to include closed-form solution
def train_all_models():
    models = [
        closed_form_linear_regression(),  # Add closed-form solution
        train_linear_regression(),
        train_lasso(),
        train_ridge(),
        train_polynomial(),
        train_rbf()
    ]
    return pd.DataFrame([m for m in models if m is not None])

# Add function to compare closed-form with gradient descent
def compare_linear_methods(results_df):
    """Compare closed-form solution with gradient descent method"""
    closed_form_results = results_df[results_df['Model'] == 'Closed-Form Linear Regression'].iloc[0]
    gradient_descent_results = results_df[results_df['Model'] == 'Linear Regression'].iloc[0]

    print("\nComparison: Closed-Form vs Gradient Descent")
    print("-" * 50)
    metrics = ['MSE', 'MAE', 'R2', 'RMSE']

    comparison = pd.DataFrame({
        'Metric': metrics,
        'Closed-Form': [closed_form_results[m] for m in metrics],
        'Gradient Descent': [gradient_descent_results[m] for m in metrics]
    })

    print(comparison)

    # Calculate relative difference
    print("\nRelative Difference in Performance:")
    for metric in metrics:
        cf_val = closed_form_results[metric]
        gd_val = gradient_descent_results[metric]
        rel_diff = abs(cf_val - gd_val) / abs(cf_val) * 100
        print(f"{metric}: {rel_diff:.4f}%")

# Modify the main execution code
print("\nTraining all models (including closed-form solution)...")
results_df = train_all_models()
print("\nModel Comparison on Validation Set:")
print(results_df.sort_values('MSE')[['Model', 'MSE', 'MAE', 'R2', 'RMSE']])

# Compare linear methods
compare_linear_methods(results_df)

def forward_feature_selection(X_train, X_val, y_train, y_val, max_features=None, min_improvement=0.001):
    """
    Implement forward feature selection with proper improvement calculation
    """
    print("\nStarting Forward Feature Selection...")

    available_features = list(X_train.columns)
    selected_features = []
    current_best_mse = float('inf')  # Start with worst possible MSE
    feature_history = []

    if max_features is None:
        max_features = len(available_features)

    def evaluate_feature_set(feature_set):
        if not feature_set:
            return float('inf'), 0, None

        model = LinearRegression()
        model.fit(X_train[feature_set], y_train)
        y_pred = model.predict(X_val[feature_set])
        mse = mean_squared_error(y_val, y_pred)
        r2 = r2_score(y_val, y_pred)

        return mse, r2, model

    print("\nFeature Selection Progress:")
    print("-" * 60)

    # Main selection loop
    while len(selected_features) < max_features:
        best_new_mse = float('inf')
        best_new_r2 = 0
        best_feature = None

        # Try each remaining feature
        for feature in available_features:
            current_features = selected_features + [feature]
            mse, r2, _ = evaluate_feature_set(current_features)

            if mse < best_new_mse:
                best_new_mse = mse
                best_new_r2 = r2
                best_feature = feature

        # Calculate relative improvement in MSE
        if current_best_mse == float('inf'):
            improvement = 1.0  # First feature
        else:
            improvement = (current_best_mse - best_new_mse) / current_best_mse

        # Record selection step
        feature_history.append({
            'Step': len(selected_features) + 1,
            'Feature': best_feature,
            'MSE': best_new_mse,
            'R2': best_new_r2,
            'Improvement': improvement
        })

        # Print progress
        print(f"\nStep {len(selected_features) + 1}:")
        print(f"Added feature: {best_feature}")
        print(f"MSE: {best_new_mse:.2f}")
        print(f"R² Score: {best_new_r2:.4f}")
        print(f"Relative Improvement: {improvement:.4%}")

        # Check stopping criterion
        if improvement < min_improvement and len(selected_features) > 0:
            print(f"\nStopping: Improvement ({improvement:.4%}) below threshold ({min_improvement})")
            break

        # Update selections
        selected_features.append(best_feature)
        available_features.remove(best_feature)
        current_best_mse = best_new_mse

        if not available_features:
            break

    # Create feature importance summary
    importance_df = pd.DataFrame(feature_history)
    total_improvement = importance_df['Improvement'].sum()
    importance_df['Relative_Importance'] = importance_df['Improvement'] / total_improvement * 100

    print("\nFinal Selection Results:")
    print("-" * 60)
    print(f"Number of selected features: {len(selected_features)}")
    print(f"Final R² Score: {importance_df['R2'].iloc[-1]:.4f}")
    print(f"Final MSE: {importance_df['MSE'].iloc[-1]:.2f}")

    print("\nFeature Importance Summary:")
    for _, row in importance_df.iterrows():
        print(f"{row['Feature']}: {row['Relative_Importance']:.2f}% improvement")

    def get_selected_features(X):
        return X[selected_features]

    return selected_features, importance_df, get_selected_features

# Apply the fixed feature selection
selected_features, importance_df, feature_selector = forward_feature_selection(
    X_train, X_val, y_train, y_val,
    min_improvement=0.001
)

# Print final dataset shapes
print("\nDataset shapes after feature selection:")
print(f"Training set: {feature_selector(X_train).shape}")
print(f"Validation set: {feature_selector(X_val).shape}")
print(f"Test set: {feature_selector(X_test).shape}")

# Get the selected features datasets
X_train_selected = feature_selector(X_train)
X_val_selected = feature_selector(X_val)
X_test_selected = feature_selector(X_test)

print("Training models with selected features:", selected_features)

def train_all_models_selected():
    results = []

    # Linear Regression
    print("\nTraining Linear Regression...")
    model = LinearRegression()
    model.fit(X_train_selected, y_train)
    y_pred = model.predict(X_val_selected)
    results.append(evaluate_model(y_val, y_pred, "Linear Regression"))

    # LASSO
    print("\nTraining LASSO...")
    for alpha in [0.0001, 0.001, 0.01, 0.1, 1]:
        model = Lasso(alpha=alpha, max_iter=2000)
        model.fit(X_train_selected, y_train)
        y_pred = model.predict(X_val_selected)
        results.append({
            'Model': f'LASSO (alpha={alpha})',
            'MSE': mean_squared_error(y_val, y_pred),
            'MAE': mean_absolute_error(y_val, y_pred),
            'R2': r2_score(y_val, y_pred),
            'RMSE': np.sqrt(mean_squared_error(y_val, y_pred)),
            'model': model
        })

    # Ridge
    print("\nTraining Ridge...")
    for alpha in [0.0001, 0.001, 0.01, 0.1, 1]:
        model = Ridge(alpha=alpha)
        model.fit(X_train_selected, y_train)
        y_pred = model.predict(X_val_selected)
        results.append({
            'Model': f'Ridge (alpha={alpha})',
            'MSE': mean_squared_error(y_val, y_pred),
            'MAE': mean_absolute_error(y_val, y_pred),
            'R2': r2_score(y_val, y_pred),
            'RMSE': np.sqrt(mean_squared_error(y_val, y_pred)),
            'model': model
        })

    # Polynomial
    print("\nTraining Polynomial Regression...")
    for degree in [2, 3, 4]:
        try:
            poly = PolynomialFeatures(degree=degree, include_bias=True)
            X_train_poly = poly.fit_transform(X_train_selected)
            X_val_poly = poly.transform(X_val_selected)

            model = LinearRegression(fit_intercept=False)
            model.fit(X_train_poly, y_train)
            y_pred = model.predict(X_val_poly)

            results.append({
                'Model': f'Polynomial (degree={degree})',
                'MSE': mean_squared_error(y_val, y_pred),
                'MAE': mean_absolute_error(y_val, y_pred),
                'R2': r2_score(y_val, y_pred),
                'RMSE': np.sqrt(mean_squared_error(y_val, y_pred)),
                'model': model,
                'poly': poly
            })

            print(f"Degree {degree} - R2: {r2_score(y_val, y_pred):.4f}")

        except Exception as e:
            print(f"Error in polynomial degree {degree}: {str(e)}")

    # RBF
    print("\nTraining RBF Kernel Regression...")
    for alpha in [0.0001, 0.001, 0.01, 0.1]:
        for gamma in [0.0001, 0.001, 0.01, 0.1]:
            model = KernelRidge(alpha=alpha, kernel='rbf', gamma=gamma)
            model.fit(X_train_selected, y_train)
            y_pred = model.predict(X_val_selected)

            results.append({
                'Model': f'RBF (alpha={alpha}, gamma={gamma})',
                'MSE': mean_squared_error(y_val, y_pred),
                'MAE': mean_absolute_error(y_val, y_pred),
                'R2': r2_score(y_val, y_pred),
                'RMSE': np.sqrt(mean_squared_error(y_val, y_pred)),
                'model': model
            })

    return pd.DataFrame(results)

# Train all models with selected features
results_df = train_all_models_selected()

# Sort and display results
print("\nModel Comparison on Validation Set:")
results_summary = results_df.sort_values('MSE')[['Model', 'MSE', 'MAE', 'R2', 'RMSE']]
print(results_summary)

# Get best model
best_model_info = results_df.loc[results_df['MSE'].idxmin()]
print(f"\nBest model: {best_model_info['Model']}")

# Evaluate best model on test set
if 'Polynomial' in best_model_info['Model']:
    poly = best_model_info['poly']
    X_test_transformed = poly.transform(X_test_selected)
    y_pred = best_model_info['model'].predict(X_test_transformed)
else:
    y_pred = best_model_info['model'].predict(X_test_selected)

test_results = evaluate_model(y_test, y_pred, best_model_info['Model'])
print("\nBest Model Performance on Test Set:")
print(pd.DataFrame([test_results]))

# Check and process the features
def preprocess_top_features():
    # Get the original features
    X_train_check = X_train[['horse_power', 'top_speed', 'engine_capacity']]

    print("Feature Statistics:")
    print(X_train_check.describe())

    # Check for skewness
    print("\nSkewness before transformation:")
    print(X_train_check.skew())

    # Apply log transformation to highly skewed features
    X_train_processed = X_train_check.copy()
    X_val_processed = X_val[['horse_power', 'top_speed', 'engine_capacity']].copy()
    X_test_processed = X_test[['horse_power', 'top_speed', 'engine_capacity']].copy()

    # Log transform horse_power and engine_capacity if they're skewed
    for col in ['horse_power', 'engine_capacity']:
        if X_train_check[col].skew() > 1:
            X_train_processed[col] = np.log1p(X_train_check[col])
            X_val_processed[col] = np.log1p(X_val[col])
            X_test_processed[col] = np.log1p(X_test[col])

    print("\nSkewness after transformation:")
    print(X_train_processed.skew())

    # Scale features to similar ranges
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_processed)
    X_val_scaled = scaler.transform(X_val_processed)
    X_test_scaled = scaler.transform(X_test_processed)

    # Convert back to DataFrame to keep column names
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_processed.columns)
    X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_train_processed.columns)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_train_processed.columns)

    return X_train_scaled, X_val_scaled, X_test_scaled

# Process the data
X_train_processed, X_val_processed, X_test_processed = preprocess_top_features()

print("\nCorrelation matrix after processing:")
print(X_train_processed.corr())

# Check relationship with target variable
print("\nCorrelation with price:")
for col in X_train_processed.columns:
    correlation = np.corrcoef(X_train_processed[col], y_train)[0,1]
    print(f"{col}: {correlation:.4f}")

def train_models_with_processed_features():
    # Polynomial features with interaction terms
    poly = PolynomialFeatures(degree=2, include_bias=False)
    X_train_poly = poly.fit_transform(X_train_processed)
    X_val_poly = poly.transform(X_val_processed)

    # Train polynomial regression
    poly_model = LinearRegression()
    poly_model.fit(X_train_poly, y_train)
    poly_pred = poly_model.predict(X_val_poly)

    # Train RBF with optimized parameters
    rbf_model = KernelRidge(
        alpha=0.0001,
        kernel='rbf',
        gamma=0.1
    )
    rbf_model.fit(X_train_processed, y_train)
    rbf_pred = rbf_model.predict(X_val_processed)

    # Compare results
    print("\nPolynomial Regression Results:")
    print(evaluate_model(y_val, poly_pred, "Polynomial Regression"))

    print("\nRBF Regression Results:")
    print(evaluate_model(y_val, rbf_pred, "RBF Regression"))

    # Print feature importance for polynomial model
    feature_names = poly.get_feature_names_out(['horse_power', 'top_speed', 'engine_capacity'])
    importance = pd.DataFrame({
        'Feature': feature_names,
        'Coefficient': poly_model.coef_
    })
    print("\nPolynomial Feature Importance:")
    print(importance.sort_values('Coefficient', key=abs, ascending=False))

# Train models with processed features
train_models_with_processed_features()

def train_optimized_model():
    # Create interaction features
    X_train_opt = X_train_processed.copy()
    X_val_opt = X_val_processed.copy()
    X_test_opt = X_test_processed.copy()

    # Add polynomial terms for horse_power (most important feature)
    X_train_opt['horse_power_sq'] = X_train_opt['horse_power'] ** 2
    X_val_opt['horse_power_sq'] = X_val_opt['horse_power'] ** 2
    X_test_opt['horse_power_sq'] = X_test_opt['horse_power'] ** 2

    # Add interaction between top features
    X_train_opt['power_speed'] = X_train_opt['horse_power'] * X_train_opt['top_speed']
    X_val_opt['power_speed'] = X_val_opt['horse_power'] * X_val_opt['top_speed']
    X_test_opt['power_speed'] = X_test_opt['horse_power'] * X_test_opt['top_speed']

    # Train models with optimized parameters based on feature relationships

    # 1. RBF with optimized parameters
    rbf_model = KernelRidge(
        alpha=0.0001,  # Small alpha due to strong correlations
        kernel='rbf',
        gamma=0.1      # Moderate gamma for balance
    )

    # 2. Polynomial with selected terms
    poly = PolynomialFeatures(degree=2, interaction_only=True)

    # Train and evaluate models
    models = {
        'RBF': (rbf_model, X_train_opt, X_val_opt),
        'Polynomial': (LinearRegression(),
                      poly.fit_transform(X_train_opt),
                      poly.transform(X_val_opt))
    }

    results = {}
    for name, (model, X_tr, X_v) in models.items():
        # Train
        model.fit(X_tr, y_train)

        # Predict
        y_pred = model.predict(X_v)

        # Evaluate
        metrics = evaluate_model(y_val, y_pred, name)
        results[name] = {
            'metrics': metrics,
            'model': model,
            'predictions': y_pred
        }

        print(f"\n{name} Results:")
        print(metrics)

        # Print feature importances for polynomial
        if name == 'Polynomial':
            feature_names = poly.get_feature_names_out(X_train_opt.columns)
            importances = pd.DataFrame({
                'Feature': feature_names,
                'Coefficient': abs(model.coef_)
            }).sort_values('Coefficient', ascending=False)
            print("\nTop Feature Importances:")
            print(importances.head(10))

    return results, X_train_opt, X_val_opt, X_test_opt

# Train optimized models
results, X_train_opt, X_val_opt, X_test_opt = train_optimized_model()

# Analyze residuals
def analyze_residuals(results):
    for name, result in results.items():
        residuals = y_val - result['predictions']

        print(f"\n{name} Residual Analysis:")
        print(f"Mean residual: {residuals.mean():.2f}")
        print(f"Std residual: {residuals.std():.2f}")
        print(f"Residual range: [{residuals.min():.2f}, {residuals.max():.2f}]")

analyze_residuals(results)

from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

def tune_hyperparameters():
    """
    Perform grid search for hyperparameter tuning on all models
    """
    results = {}

    # 1. LASSO with Grid Search
    print("\nTuning LASSO Regression...")
    lasso_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('lasso', Lasso())
    ])

    lasso_param_grid = {
        'lasso__alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0],
        'lasso__max_iter': [3000],
        'lasso__tol': [1e-4, 1e-3],
        'lasso__selection': ['cyclic', 'random']
    }

    lasso_grid = GridSearchCV(
        lasso_pipeline,
        lasso_param_grid,
        cv=5,
        scoring=['neg_mean_squared_error', 'r2'],
        refit='neg_mean_squared_error',
        verbose=1,
        n_jobs=-1,
        return_train_score=True
    )

    lasso_grid.fit(X_train, y_train)
    results['LASSO'] = {
        'best_params': lasso_grid.best_params_,
        'best_score': -lasso_grid.best_score_,
        'model': lasso_grid.best_estimator_
    }

    # 2. Ridge with Grid Search
    print("\nTuning Ridge Regression...")
    ridge_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('ridge', Ridge())
    ])

    ridge_param_grid = {
        'ridge__alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0],
        'ridge__tol': [1e-4, 1e-3],
        'ridge__solver': ['auto', 'svd', 'sag']
    }

    ridge_grid = GridSearchCV(
        ridge_pipeline,
        ridge_param_grid,
        cv=5,
        scoring=['neg_mean_squared_error', 'r2'],
        refit='neg_mean_squared_error',
        verbose=1,
        n_jobs=-1,
        return_train_score=True
    )

    ridge_grid.fit(X_train, y_train)
    results['Ridge'] = {
        'best_params': ridge_grid.best_params_,
        'best_score': -ridge_grid.best_score_,
        'model': ridge_grid.best_estimator_
    }

    # 3. Polynomial Regression with Grid Search
    print("\nTuning Polynomial Regression...")
    poly_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('poly', PolynomialFeatures()),
        ('regressor', LinearRegression())
    ])

    poly_param_grid = {
        'poly__degree': [2, 3, 4],
        'poly__interaction_only': [True, False],
        'regressor__fit_intercept': [True, False]
    }

    poly_grid = GridSearchCV(
        poly_pipeline,
        poly_param_grid,
        cv=5,
        scoring=['neg_mean_squared_error', 'r2'],
        refit='neg_mean_squared_error',
        verbose=1,
        n_jobs=-1,
        return_train_score=True
    )

    poly_grid.fit(X_train, y_train)
    results['Polynomial'] = {
        'best_params': poly_grid.best_params_,
        'best_score': -poly_grid.best_score_,
        'model': poly_grid.best_estimator_
    }

    # 4. RBF Kernel Ridge with Grid Search
    print("\nTuning RBF Kernel Ridge...")
    rbf_pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('rbf', KernelRidge(kernel='rbf'))
    ])

    rbf_param_grid = {
        'rbf__alpha': [0.0001, 0.001, 0.01, 0.1, 1.0],
        'rbf__gamma': [0.0001, 0.001, 0.01, 0.1, 1.0]
    }

    rbf_grid = GridSearchCV(
        rbf_pipeline,
        rbf_param_grid,
        cv=5,
        scoring=['neg_mean_squared_error', 'r2'],
        refit='neg_mean_squared_error',
        verbose=1,
        n_jobs=-1,
        return_train_score=True
    )

    rbf_grid.fit(X_train, y_train)
    results['RBF'] = {
        'best_params': rbf_grid.best_params_,
        'best_score': -rbf_grid.best_score_,
        'model': rbf_grid.best_estimator_
    }

    return results

def evaluate_tuned_models(tuned_results):
    """
    Evaluate and compare tuned models
    """
    evaluation_results = []

    for model_name, model_info in tuned_results.items():
        model = model_info['model']
        val_pred = model.predict(X_val)
        test_pred = model.predict(X_test)

        val_metrics = {
            'Model': model_name,
            'Validation_MSE': mean_squared_error(y_val, val_pred),
            'Validation_R2': r2_score(y_val, val_pred),
            'Test_MSE': mean_squared_error(y_test, test_pred),
            'Test_R2': r2_score(y_test, test_pred),
            'Best_Parameters': model_info['best_params']
        }
        evaluation_results.append(val_metrics)

    return pd.DataFrame(evaluation_results)

# Perform hyperparameter tuning
print("Starting hyperparameter tuning...")
tuned_results = tune_hyperparameters()

# Evaluate tuned models
print("\nEvaluating tuned models...")
evaluation_df = evaluate_tuned_models(tuned_results)

# Display results
print("\nFinal Results After Hyperparameter Tuning:")
print("="*100)
for model_name in evaluation_df['Model']:
    model_results = evaluation_df[evaluation_df['Model'] == model_name].iloc[0]
    print(f"\n{model_name} Results:")
    print("-"*50)
    print(f"Validation MSE: {model_results['Validation_MSE']:,.2f}")
    print(f"Validation R²:  {model_results['Validation_R2']:.4f}")
    print(f"Test MSE:      {model_results['Test_MSE']:,.2f}")
    print(f"Test R²:       {model_results['Test_R2']:.4f}")
    print("\nBest Parameters:")
    for param, value in model_results['Best_Parameters'].items():
        print(f"{param}: {value}")
    print("-"*50)

# Identify best model
best_model = evaluation_df.loc[evaluation_df['Test_MSE'].idxmin()]
print("\nBest Overall Model:")
print(f"Model: {best_model['Model']}")
print(f"Test MSE: {best_model['Test_MSE']:,.2f}")
print(f"Test R²:  {best_model['Test_R2']:.4f}")